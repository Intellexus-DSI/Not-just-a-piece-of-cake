{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17cc0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" #This should be before import of tokenizer\n",
    "import sys\n",
    "from datetime import datetime \n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Union\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import (\n",
    "    BertForTokenClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    RobertaTokenizerFast,\n",
    "    DataCollatorForTokenClassification,\n",
    "    set_seed,\n",
    ")\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import json\n",
    "\n",
    "\n",
    "# Get the project root directory\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parent  # Go up one level from encoder_fine_tuning to cross-lingual-idioms\n",
    "sys.path.append(str(project_root))\n",
    "from src.utils import get_data\n",
    "\n",
    "GPU = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU\n",
    "\n",
    "TOTAL_RECORDS_NUM = 60000\n",
    "SEEDS = [5, 7]#, 123, 1773] #42 is done\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "JUMP = 0.1 #Usually 0.1 or 0.2\n",
    "JUMP_STR = str(JUMP).replace('.', '_') #To be used in file paths\n",
    "\n",
    "\n",
    "print(f\"{datetime.now()} Start gpu {GPU}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871e556-5064-44d5-b2fd-850b3a55ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Set up ###########################\n",
    "\n",
    "class TaskConfig(Enum):\n",
    "    DODIOM = \"dodiom\"\n",
    "    ID10M = \"id10m\"\n",
    "    OPEN_MWE = \"open_mwe\"\n",
    "    MAGPIE = \"magpie\"\n",
    "\n",
    "def get_idiom_only_list() -> List[TaskConfig]:\n",
    "    return [TaskConfig.ID10M.value]\n",
    "\n",
    "\n",
    "LANGUAGE_TO_CODE: Dict[str, str] = {\n",
    "    \"english\": \"EN\",\n",
    "    \"spanish\": \"ES\",\n",
    "    \"german\": \"DE\",\n",
    "    \"japanese\": \"JP\",\n",
    "    \"turkish\": \"TR\",\n",
    "    \"chinese\": \"ZH\",\n",
    "    \"french\": \"FR\",\n",
    "    \"polish\": \"PL\",\n",
    "    \"italian\": \"IT\",\n",
    "    \"dutch\": \"NL\",\n",
    "    \"portuguese\": \"PT\"\n",
    "}\n",
    "\n",
    "def get_language_map() -> Dict[str, str]:\n",
    "    CODE_TO_LANGUAGE: Dict[str, str] = {v: k for k, v in LANGUAGE_TO_CODE.items()}\n",
    "    \n",
    "    # Merge into a single bidirectional object\n",
    "    return {**LANGUAGE_TO_CODE, **CODE_TO_LANGUAGE}\n",
    "\n",
    "LANGUAGE_MAP = get_language_map()\n",
    "\n",
    "\n",
    "LANG_TO_SOURCES = defaultdict(get_idiom_only_list)\n",
    "LANG_TO_SOURCES[\"turkish\"] = [TaskConfig.DODIOM.value]\n",
    "LANG_TO_SOURCES[\"japanese\"] = [TaskConfig.OPEN_MWE.value]\n",
    "# Optional: LANG_TO_SOURCES[\"english\"].append(TaskConfig.MAGPIE.value)\n",
    "\n",
    "OOD_ONE_OUT_LANG = \"japanese\"\n",
    "OTHER_LANGS = list(LANGUAGE_TO_CODE.keys())\n",
    "OTHER_LANGS.remove(OOD_ONE_OUT_LANG)\n",
    "ODD_ONE_DATASET = get_data(lang=OOD_ONE_OUT_LANG, task=LANG_TO_SOURCES[OOD_ONE_OUT_LANG][0]) #Taking only from the first source of OOD_ONE_OUT_LANG\n",
    "OTHER_LANGS_TO_DATAFRAMES_TRAIN_ONLY = defaultdict(lambda: pd.DataFrame())\n",
    "RESULTS_DIR = Path(f\"odd_one_out_{OOD_ONE_OUT_LANG}_records_num_{TOTAL_RECORDS_NUM}_jump_{JUMP_STR}_results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True) \n",
    "\n",
    "for otra_lengua in OTHER_LANGS:\n",
    "    for src in LANG_TO_SOURCES[otra_lengua]:\n",
    "        cur_df = get_data(lang=otra_lengua, task=src)[\"train\"]\n",
    "        OTHER_LANGS_TO_DATAFRAMES_TRAIN_ONLY[otra_lengua] = pd.concat([OTHER_LANGS_TO_DATAFRAMES_TRAIN_ONLY[otra_lengua], cur_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ac025-fd8f-4a89-abab-a4b0beb72029",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### auxiliary ###########################\n",
    "BEST_F1_PER_TRAINING = -1\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(-1)\n",
    "\n",
    "    true_labels, pred_labels = [], []\n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        for pred_id, label_id in zip(pred_seq, label_seq):\n",
    "            if label_id == -100:\n",
    "                continue\n",
    "            true_labels.append(id2label[label_id])\n",
    "            pred_labels.append(id2label[pred_id])\n",
    "        \n",
    "    cur_f1 = f1_score(true_labels, pred_labels, average=\"macro\", zero_division=0)\n",
    "    global BEST_F1_PER_TRAINING\n",
    "    BEST_F1_PER_TRAINING = max(BEST_F1_PER_TRAINING, cur_f1)\n",
    "    return {\n",
    "        \"eval_f1\": cur_f1,\n",
    "        \"precision\": precision_score(true_labels, pred_labels, average=\"macro\", zero_division=0),\n",
    "        \"recall\": recall_score(true_labels, pred_labels, average=\"macro\", zero_division=0),\n",
    "        \"accuracy\": accuracy_score(true_labels, pred_labels),\n",
    "    }\n",
    "\n",
    "\n",
    "def encode_labels(example):\n",
    "    example[\"labels\"] = [label2id[tag] for tag in example[\"tags\"]]\n",
    "    return example\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(example[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "    word_ids = tokenized.word_ids()\n",
    "    previous_word_idx = None\n",
    "    labels = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"labels\"][word_idx])\n",
    "        else:\n",
    "            labels.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels_slow(example):\n",
    "    # For slow tokenizers, we need to align manually\n",
    "    # Tokenize each word and assign the label to all resulting tokens\n",
    "    tokens = example[\"tokens\"]\n",
    "    labels = example[\"labels\"]\n",
    "    tokenized_inputs = tokenizer(\n",
    "        tokens,\n",
    "        is_split_into_words=True,  # For slow tokenizer, this works as expected\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,  # or whatever length you need\n",
    "    )\n",
    "    # Manually align labels\n",
    "    word_ids = []\n",
    "    cur = 0\n",
    "    for token in tokenized_inputs[\"input_ids\"]:\n",
    "        # For slow tokenizers, you can align using the tokens list\n",
    "        # Here, we assign the label of the current word to all its sub-tokens\n",
    "        # This is a simplification; you may need to adjust for special tokens\n",
    "        if cur < len(labels):\n",
    "            word_ids.append(labels[cur])\n",
    "            cur += 1\n",
    "        else:\n",
    "            word_ids.append(-100)  # Padding label for special tokens\n",
    "    tokenized_inputs[\"labels\"] = word_ids[:len(tokenized_inputs[\"input_ids\"])]\n",
    "    return tokenized_inputs\n",
    "\n",
    "def get_tokenized_dataset(lang: str, model_name: str, dataset: DatasetDict) -> DatasetDict:\n",
    "    if \"japanese\" in model_name or lang==\"japanese\":\n",
    "        return dataset.map(tokenize_and_align_labels_slow, batched=False)\n",
    "    else:\n",
    "        return dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "\n",
    "def get_model_by_name(lang: str, model_name: str, label2id: Dict[str, int], id2label: Dict[int, str], hidden_dropout_prob: float=0.5,\n",
    "                      attention_probs_dropout_prob: float=0.5) -> Union[BertForTokenClassification, AutoModelForTokenClassification]:\n",
    "    if model_name in [\"FacebookAI/xlm-roberta-base\", \"FacebookAI/roberta-base\"] or \"japanese\" in model_name or lang==\"japanese\":\n",
    "        return AutoModelForTokenClassification.from_pretrained(model_name,\n",
    "            num_labels=len(label2id),\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,)\n",
    "    else:\n",
    "        return BertForTokenClassification.from_pretrained(model_name,\n",
    "            num_labels=len(label2id),\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "        )\n",
    "\n",
    "def get_tokenizer_by_model_name(lang: str, model_name: str) -> Union[AutoTokenizer, RobertaTokenizerFast]:\n",
    "    if model_name == \"FacebookAI/roberta-base\":\n",
    "        return RobertaTokenizerFast.from_pretrained('roberta-base', add_prefix_space=True)\n",
    "    elif \"japanese\" in model_name or lang==\"japanese\":\n",
    "        return AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "    else:\n",
    "        return AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5164b9-f69a-48de-95e7-ae18cdd6c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Logic ###########################\n",
    "\n",
    "def finetune(train_df: pd.DataFrame, cur_seed: int, model_name: str = MODEL_NAME):\n",
    "    set_seed(cur_seed)\n",
    "    train_dataset = Dataset.from_pandas(train_df[[\"tokens\", \"tags\"]])\n",
    "    test_dataset = Dataset.from_pandas(ODD_ONE_DATASET[\"test\"][[\"tokens\", \"tags\"]])\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    })\n",
    "\n",
    "    global label2id, id2label, tokenizer  # used in encode_labels and compute_metrics\n",
    "    unique_tags = sorted(set(tag for row in dataset[\"train\"][\"tags\"] for tag in row))\n",
    "    label2id = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "    id2label = {i: tag for tag, i in label2id.items()}\n",
    "    tokenizer = get_tokenizer_by_model_name(OOD_ONE_OUT_LANG, MODEL_NAME)#AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    dataset = dataset.map(encode_labels)\n",
    "    \n",
    "    tokenized_dataset = get_tokenized_dataset(OOD_ONE_OUT_LANG, MODEL_NAME, dataset) #tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "    model = get_model_by_name(OOD_ONE_OUT_LANG, MODEL_NAME, label2id, id2label)\n",
    "  \n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"odd_one_out_out\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",#\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=20,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=0,\n",
    "        logging_dir=\"./odd_one_out_logs\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        logging_steps=1000,\n",
    "        # save_total_limit=2,\n",
    "        seed=cur_seed,\n",
    "        dataloader_num_workers=4,\n",
    "        disable_tqdm=False,\n",
    "        report_to=[],\n",
    "        gradient_accumulation_steps=4,\n",
    "        gradient_checkpointing=False,\n",
    "        fp16=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.999,\n",
    "        adam_epsilon=1e-8,\n",
    "        max_grad_norm=1.0,\n",
    "        # load_best_model_at_end=True,\n",
    "        # metric_for_best_model=\"eval_f1\",\n",
    "        # greater_is_better=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        processing_class=tokenizer,#tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "def decimal_to_percentage(decimal: float) -> int:\n",
    "    if not (0 <= decimal <= 1):\n",
    "        raise ValueError(\"Input must be between 0 and 1\")\n",
    "    return int(round(decimal * 100))\n",
    "\n",
    "def get_run_result_path(seed: int, percentage: int) -> str:\n",
    "    return Path(RESULTS_DIR, f\"seed_{cur_seed}_odd_one_out_{OOD_ONE_OUT_LANG}_{percentage}_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eed4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_summary = {}\n",
    "perc_range = np.arange(0, 1+JUMP, JUMP)\n",
    "for i_seed, cur_seed in enumerate(SEEDS):\n",
    "    random.seed(cur_seed)\n",
    "    for i_perc, odd_one_percentage in enumerate(perc_range):\n",
    "        print(f\"{datetime.now()} Odd one out language {OOD_ONE_OUT_LANG} percentage {odd_one_percentage} ({i_perc+1}/{len(perc_range)}) cur_seed {cur_seed} ({i_seed+1}/{len(SEEDS)})\")\n",
    "        run_result_path = get_run_result_path(cur_seed, decimal_to_percentage(odd_one_percentage))\n",
    "        if os.path.exists(run_result_path) and os.path.getsize(run_result_path) > 0:\n",
    "            print(f\"{datetime.now()} Non-empty results file exists for this test {run_result_path} Skipping\")\n",
    "            continue\n",
    "\n",
    "        odd_one_samples_num = int(odd_one_percentage*TOTAL_RECORDS_NUM)\n",
    "        assert odd_one_samples_num<=len(ODD_ONE_DATASET[\"train\"]), f'{OOD_ONE_OUT_LANG} has {len(ODD_ONE_DATASET[\"train\"])} samples but {odd_one_samples_num} needed'\n",
    "        odd_one_out_samples = ODD_ONE_DATASET[\"train\"].sample(n=odd_one_samples_num, random_state=cur_seed)\n",
    "        other_langs_samples_count = TOTAL_RECORDS_NUM - len(odd_one_out_samples)\n",
    "\n",
    "        #TODO - Make sure that every language has enough samples to give. If not, adjust accordingly. If all of the together don't have enough - throw excpetion\n",
    "        n_dfs = len(OTHER_LANGS_TO_DATAFRAMES_TRAIN_ONLY)\n",
    "        base_num = other_langs_samples_count // n_dfs\n",
    "        lang_to_samples_num = {other_l: base_num for other_l in OTHER_LANGS}\n",
    "        remainder = other_langs_samples_count % n_dfs\n",
    "        if remainder>0:\n",
    "            langs_to_add = random.sample(OTHER_LANGS, remainder)\n",
    "            for l in langs_to_add:\n",
    "                lang_to_samples_num[l]+=1\n",
    "\n",
    "        print(f\"{datetime.now()} Number of samples per lang: {OOD_ONE_OUT_LANG} {len(odd_one_out_samples)} {lang_to_samples_num}\")\n",
    "\n",
    "        train_dfs = []\n",
    "        for l in OTHER_LANGS:\n",
    "            train_dfs.append(OTHER_LANGS_TO_DATAFRAMES_TRAIN_ONLY[l].sample(n=lang_to_samples_num[l], random_state=cur_seed))\n",
    "            \n",
    "        train_dfs.append(odd_one_out_samples)\n",
    "        concatenated_train_df = pd.concat(train_dfs)    \n",
    "\n",
    "        ###### ADDED\n",
    "        # Shuffle the concatenated dataframe\n",
    "        concatenated_train_df = concatenated_train_df.sample(frac=1, random_state=cur_seed).reset_index(drop=True)\n",
    "        ######\n",
    "\n",
    "        assert len(concatenated_train_df) == TOTAL_RECORDS_NUM, f\"Bug in code! Should've {TOTAL_RECORDS_NUM} samples, but have {len(concatenated_train_df)}\"\n",
    "        global BEST_F1_PER_TRAINING\n",
    "        BEST_F1_PER_TRAINING = -1\n",
    "        finetune(concatenated_train_df, cur_seed)\n",
    "        with open(run_result_path, 'w') as json_file:\n",
    "            json.dump(BEST_F1_PER_TRAINING, json_file, indent=4) \n",
    "        \n",
    "        results_summary[f\"seed_{cur_seed}_odd_lang_{OOD_ONE_OUT_LANG}_percentage_{decimal_to_percentage(odd_one_percentage)}\"] = BEST_F1_PER_TRAINING\n",
    "\n",
    "if len(results_summary)>0:\n",
    "    with open(Path(RESULTS_DIR, f\"odd_one_out_seed_{cur_seed}_{OOD_ONE_OUT_LANG}_results.json\"), 'w') as json_file:\n",
    "        json.dump(results_summary, json_file, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ae428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHow the last 10 samples\n",
    "for i, row in concatenated_train_df.tail(20).iterrows():\n",
    "    print(row[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaaa683-cf8d-4301-94d9-f9aa79e19dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{datetime.now()} FIN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mwe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
