{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8ccc6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import gc\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.utils import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b07718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: 0\n"
     ]
    }
   ],
   "source": [
    "# Set device to be CUDA \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"  # Adjust based on your available GPUs\n",
    "print(\"Using CUDA device:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = \"italian\"\n",
    "TASK = \"dodiom\"\n",
    "SPLIT = \"train\"  # \"train\" or \"test\"\n",
    "\n",
    "NUM_SAMPLES = 100  # 0 for all\n",
    "# MODEL_PATH = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "MODEL_PATH = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "BSZ = 16\n",
    "CHECKPOINT_INTERVAL = 20  \n",
    "LAYER_WISE_PROBING = True  # set to False to use single-layer span vector\n",
    "NO_GAP_WORDS = True  # If True, will only consider words in the idiom that are not separated by gaps (e.g., \"kick the bucket\" -> \"kick bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4c926d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_str = \"all\" if NUM_SAMPLES <= 0 else str(NUM_SAMPLES)\n",
    "\n",
    "output_dir = os.path.join(OUTPUT_DIR, LANG, TASK)\n",
    "model_name = MODEL_PATH.split(\"/\")[-1]\n",
    "layer_wise = \"all_hidden\" if LAYER_WISE_PROBING else \"last_hidden\"\n",
    "output_file = os.path.join(output_dir, f\"{layer_wise}_{SPLIT}_{num_samples_str}_{model_name}.jsonl\")\n",
    "output_file_vectors = os.path.join(output_dir, f\"{layer_wise}_{SPLIT}_{num_samples_str}_{model_name}_vectors.pt\")\n",
    "\n",
    "# Make sure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Check if files exist and load existing data\n",
    "existing_ids = set()\n",
    "existing_vectors = torch.tensor([])  # Initialize as empty tensor\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"Found existing output file: {output_file}\")\n",
    "    existing_data = pd.read_json(output_file, lines=True)\n",
    "    existing_ids = set(existing_data['id'].tolist())\n",
    "    print(f\"Found {len(existing_ids)} existing processed samples\")\n",
    "    \n",
    "    if os.path.exists(output_file_vectors):\n",
    "        print(f\"Found existing vectors file: {output_file_vectors}\")\n",
    "        existing_vectors = torch.load(output_file_vectors)\n",
    "        print(f\"Loaded {existing_vectors.shape[0]} existing vectors\")\n",
    "    else:\n",
    "        print(\"Warning: JSONL file exists but vectors file is missing. Starting fresh.\")\n",
    "        existing_ids = set()\n",
    "        # Create an empty tensor for vectors with 3 dimensions\n",
    "        existing_vectors = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "988f4463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded train for task='dodiom', lang='italian': (7033, 10)\n",
      "✅ Loaded test for task='dodiom', lang='italian': (500, 10)\n",
      "✅ Loaded validation for task='dodiom', lang='italian': empty\n"
     ]
    }
   ],
   "source": [
    "# Get data \n",
    "data = get_data(lang=LANG, task=TASK)[SPLIT]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cf443883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_japanese_idiom_tokens(row):\n",
    "    tokens = row[\"tokens\"]\n",
    "    pie = row[\"pie\"]\n",
    "    sentence = row[\"sentence\"]\n",
    "\n",
    "\n",
    "    # Find idiom char span\n",
    "    match = re.search(re.escape(pie), sentence)\n",
    "    if not match:\n",
    "        print(f\"Idiom '{pie}' not found in sentence: {sentence}\")\n",
    "        return []\n",
    "    idiom_start = match.start()\n",
    "    idiom_end = match.end()\n",
    "\n",
    "    # Compute character span for each token\n",
    "    token_char_spans = []\n",
    "    cursor = 0\n",
    "    for token in tokens:\n",
    "        start = cursor\n",
    "        end = cursor + len(token)\n",
    "        token_char_spans.append((start, end))\n",
    "        cursor = end\n",
    "\n",
    "    # Select tokens whose span overlaps the idiom span\n",
    "    selected_tokens = [\n",
    "        tokens[i]\n",
    "        for i, (start, end) in enumerate(token_char_spans)\n",
    "        if not (end <= idiom_start or start >= idiom_end)\n",
    "    ]\n",
    "\n",
    "    # Select tokens that are in the idiom span\n",
    "    # selected_tokens = [\n",
    "    #     token for token in selected_tokens if token in pie\n",
    "    # ]\n",
    "\n",
    "    return selected_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b20be30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Number of samples: 7033\n"
     ]
    }
   ],
   "source": [
    "if TASK == \"magpie\":\n",
    "    # # Remove idiom_tokens\n",
    "        data = data.drop(columns=[\"idiom_tokens\"], errors='ignore')\n",
    "        # New idiom_tokens is created from pie_tokens\n",
    "        data = data.rename(columns={\"pie_tokens\": \"idiom_tokens\"})\n",
    "        data[\"idiomatic\"] = data[\"label\"].apply(lambda x: True if x == \"idiomatic\" else False)\n",
    "elif TASK == \"dodiom\":\n",
    "    # Add fake id column\n",
    "    data[\"id\"] = data.index\n",
    "    # Add split column\n",
    "    data[\"split\"] = SPLIT\n",
    "    # Rename columns\n",
    "    data = data.rename(columns={\"idiom\": \"idiom_base\", \"idiom_words\": \"idiom_tokens\"})\n",
    "\n",
    "    # Add pie column\n",
    "    data[\"pie\"] = data[\"true_idioms\"].apply(lambda x: x[0])\n",
    "\n",
    "    # Add idiomatic column\n",
    "    data[\"idiomatic\"] = data[\"category\"].apply(lambda x: True if x == \"idiom\" else False)\n",
    "\n",
    "elif TASK == \"open_mwe\":\n",
    "    # Add fake id column\n",
    "    data[\"id\"] = data.index\n",
    "    # Add split column\n",
    "    data[\"split\"] = SPLIT\n",
    "    # Create new column idiom_tokens\n",
    "    data['idiom_tokens'] = data.apply(extract_japanese_idiom_tokens, axis=1)\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unsupported task: {TASK}\")\n",
    "\n",
    "# Keep only necessary columns and store full dataset\n",
    "# full_data = data[[\"id\", \"split\", \"sentence\", \"idiom_base\", \"pie\", \"idiomatic\", \"idiom_tokens\"]].copy()\n",
    "full_data = data.copy()\n",
    "\n",
    "print(\"Data loaded. Number of samples:\", len(full_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7a1176e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample results:\n",
      "Row 0:\n",
      "  Original tokens: ['A', 'Clelia', 'piace', 'passeggiare', 'per', 'i', 'campi', 'ed', 'acchiappare', 'quante', 'più', 'farfalle', 'possibile', 'con', 'il', 'suo', 'retino', ',', 'per', 'poi', 'lasciarle', 'libere']...\n",
      "  Idiom span (pie): acchiappare farfalle\n",
      "  Extracted idiom tokens: ['acchiappare', 'farfalle']\n",
      "\n",
      "Row 1:\n",
      "  Original tokens: ['A', 'me', 'sembra', 'si', 'stare', 'acchiappando', 'solo', 'farfalle', 'seguendo', 'i', 'tuoi', 'consigli']...\n",
      "  Idiom span (pie): acchiappando farfalle\n",
      "  Extracted idiom tokens: ['acchiappando', 'farfalle']\n",
      "\n",
      "Row 2:\n",
      "  Original tokens: ['Al', 'posto', 'di', 'acchiappare', 'farfalle', ',', 'pensa', 'a', 'studiare', '!']...\n",
      "  Idiom span (pie): acchiappare farfalle\n",
      "  Extracted idiom tokens: ['acchiappare', 'farfalle']\n",
      "\n",
      "Row 3:\n",
      "  Original tokens: ['Basta', 'acchiappare', 'le', 'farfalle']...\n",
      "  Idiom span (pie): acchiappare farfalle\n",
      "  Extracted idiom tokens: ['acchiappare', 'farfalle']\n",
      "\n",
      "Row 4:\n",
      "  Original tokens: ['Chi', 'vuole', 'giocare', 'ad', 'acchiappare', 'farfalle', '?']...\n",
      "  Idiom span (pie): acchiappare farfalle\n",
      "  Extracted idiom tokens: ['acchiappare', 'farfalle']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample results:\")\n",
    "for i in range(min(5, len(data))):\n",
    "    print(f\"Row {i}:\")\n",
    "    print(f\"  Original tokens: {data.iloc[i]['tokens']}...\")  # First 10 tokens\n",
    "    print(f\"  Idiom span (pie): {data.iloc[i]['pie']}\")\n",
    "    print(f\"  Extracted idiom tokens: {data.iloc[i]['idiom_tokens']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9a98e4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size before cutting: 7033\n",
      "Data size after filtering: 100\n"
     ]
    }
   ],
   "source": [
    "if existing_ids:\n",
    "    original_size = len(full_data)\n",
    "    # Filter out already processed samples\n",
    "    data = full_data[~full_data['id'].isin(existing_ids)]\n",
    "    print(f\"Filtered out {original_size - len(data)} already processed samples\")\n",
    "    if NUM_SAMPLES > 0:\n",
    "        remaining2process = NUM_SAMPLES - len(existing_ids)\n",
    "        if remaining2process < 0:\n",
    "            print(f\"Warning: More samples already processed than requested ({len(existing_ids)} vs {NUM_SAMPLES}).\")\n",
    "            remaining2process = 0\n",
    "    else:\n",
    "        remaining2process = len(data) - len(existing_ids)\n",
    "\n",
    "    print(f\"Requested total samples to process: {NUM_SAMPLES if NUM_SAMPLES > 0 else original_size}\")\n",
    "    print(f\"Remaining samples to process: {remaining2process}\")\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        print(\"All samples already processed!\")\n",
    "        # return\n",
    "else:\n",
    "    data = full_data.copy()\n",
    "    remaining2process = NUM_SAMPLES if NUM_SAMPLES > 0 else len(data)\n",
    "\n",
    "\n",
    "# Debug, use a small subset\n",
    "if NUM_SAMPLES > 0:\n",
    "    print(\"Data size before cutting:\", len(data))\n",
    "    indices = np.random.choice(data.index, size=min(remaining2process, len(data)), replace=False)\n",
    "    data = data.loc[indices].reset_index(drop=True)\n",
    "    print(\"Data size after filtering:\", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9ff01409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idiom_base</th>\n",
       "      <th>sentence</th>\n",
       "      <th>category</th>\n",
       "      <th>idiom_indices</th>\n",
       "      <th>idiom_tokens</th>\n",
       "      <th>tokens</th>\n",
       "      <th>true_idioms</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_ids</th>\n",
       "      <th>idiom_full_span</th>\n",
       "      <th>id</th>\n",
       "      <th>split</th>\n",
       "      <th>pie</th>\n",
       "      <th>idiomatic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prendere con le pinze</td>\n",
       "      <td>Prendo quella cosa con le pinze per una maggio...</td>\n",
       "      <td>nonidiom</td>\n",
       "      <td>[0, 3, 4, 5]</td>\n",
       "      <td>[Prendo, con, le, pinze]</td>\n",
       "      <td>[Prendo, quella, cosa, con, le, pinze, per, un...</td>\n",
       "      <td>[Prendo con le pinze]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>Prendo quella cosa con</td>\n",
       "      <td>4610</td>\n",
       "      <td>train</td>\n",
       "      <td>Prendo con le pinze</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brancolare nel buio</td>\n",
       "      <td>Non brancolare così nel buio, chiedi aiuto ai ...</td>\n",
       "      <td>idiom</td>\n",
       "      <td>[1, 3, 4]</td>\n",
       "      <td>[brancolare, nel, buio]</td>\n",
       "      <td>[Non, brancolare, così, nel, buio, ,, chiedi, ...</td>\n",
       "      <td>[brancolare nel buio]</td>\n",
       "      <td>[O, B-IDIOM, I-IDIOM, I-IDIOM, O, O, O, O, O, ...</td>\n",
       "      <td>[0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>brancolare così nel</td>\n",
       "      <td>1976</td>\n",
       "      <td>train</td>\n",
       "      <td>brancolare nel buio</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              idiom_base                                           sentence  \\\n",
       "0  Prendere con le pinze  Prendo quella cosa con le pinze per una maggio...   \n",
       "1    Brancolare nel buio  Non brancolare così nel buio, chiedi aiuto ai ...   \n",
       "\n",
       "   category idiom_indices              idiom_tokens  \\\n",
       "0  nonidiom  [0, 3, 4, 5]  [Prendo, con, le, pinze]   \n",
       "1     idiom     [1, 3, 4]   [brancolare, nel, buio]   \n",
       "\n",
       "                                              tokens            true_idioms  \\\n",
       "0  [Prendo, quella, cosa, con, le, pinze, per, un...  [Prendo con le pinze]   \n",
       "1  [Non, brancolare, così, nel, buio, ,, chiedi, ...  [brancolare nel buio]   \n",
       "\n",
       "                                                tags  \\\n",
       "0                     [O, O, O, O, O, O, O, O, O, O]   \n",
       "1  [O, B-IDIOM, I-IDIOM, I-IDIOM, O, O, O, O, O, ...   \n",
       "\n",
       "                                         tag_ids         idiom_full_span  \\\n",
       "0                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  Prendo quella cosa con   \n",
       "1  [0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]     brancolare così nel   \n",
       "\n",
       "     id  split                  pie  idiomatic  \n",
       "0  4610  train  Prendo con le pinze      False  \n",
       "1  1976  train  brancolare nel buio       True  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f514f477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is loaded on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Check on which device the model is loaded\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model is loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "acec93a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Number of sentences tokenized: 100\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentences\n",
    "tokenized = tokenizer(\n",
    "    data[\"sentence\"].tolist(),\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print(\"Tokenization complete. Number of sentences tokenized:\", len(tokenized[\"input_ids\"]))\n",
    "\n",
    "# Save separately for convenience\n",
    "offset_mapping = tokenized[\"offset_mapping\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8e565f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tokenization (store offset_mapping separately)\n",
    "tokenized_inputs = {k: v for k, v in tokenized.items() if k != \"offset_mapping\"}\n",
    "keys = list(tokenized_inputs.keys())\n",
    "\n",
    "# Step 2: Batch processing using DataLoader\n",
    "dataset = TensorDataset(*[tokenized_inputs[k] for k in keys])\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BSZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "be6c6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize mean_vectors and processed_data\n",
    "mean_vectors = existing_vectors\n",
    "processed_count = len(existing_ids) if existing_ids else 0\n",
    "\n",
    "# Track all processed data for checkpointing\n",
    "if existing_ids:\n",
    "    # Load existing processed data for checkpointing\n",
    "    existing_data = pd.read_json(output_file, lines=True)\n",
    "    processed_data = existing_data.copy()\n",
    "else:\n",
    "    processed_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "15b04e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \"\"\"Normalize without over-aggressive replacements. Lowercase and strip control chars.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = text.replace(\"’\", \"\").replace(\"‘\", \"\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    text = text.replace(\"`\", \"'\").replace(\"'\", \"\")  # optionally keep apostrophes\n",
    "    text = ''.join(c for c in text if not unicodedata.category(c).startswith('C'))\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "\n",
    "def group_subword_matches(decoded_tokens, target_tokens):\n",
    "    selected_indices = []\n",
    "    i = 0\n",
    "    while i < len(decoded_tokens):\n",
    "        match_found = False\n",
    "        for j in range(len(decoded_tokens), i, -1):\n",
    "            candidate_tokens = decoded_tokens[i:j]\n",
    "            candidate_text = ''.join([t[0] for t in candidate_tokens])\n",
    "            for target in target_tokens:\n",
    "                if candidate_text == target:\n",
    "                    selected_indices.extend([t[1] for t in candidate_tokens])\n",
    "                    target_tokens.remove(target)\n",
    "                    i = j - 1\n",
    "                    match_found = True\n",
    "                    break\n",
    "            if match_found:\n",
    "                break\n",
    "        i += 1\n",
    "    return selected_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_bad_unicode(word):\n",
    "    # Fix cases like \"motionsâģļ\" => strip junk and keep core\n",
    "    return normalize(re.sub(r\"[^a-zA-Z0-9]+$\", \"\", word))\n",
    "\n",
    "def match_japanese_span_tokens(decoded_tokens: list[tuple[str, int]], span_words: list[str]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Match Japanese span by comparing the full target span string to concatenated decoded tokens.\n",
    "    \n",
    "    Parameters:\n",
    "    - decoded_tokens: list of (decoded_token, token_index)\n",
    "    - span_words: list of original words from the span (may include kana/kanji sequences)\n",
    "    \n",
    "    Returns:\n",
    "    - List of token indices that correspond to the span\n",
    "    \"\"\"\n",
    "    target_string = normalize(''.join(span_words))\n",
    "    \n",
    "    for start in range(len(decoded_tokens)):\n",
    "        for end in range(start + 1, len(decoded_tokens) + 1):\n",
    "            candidate = ''.join(tok for tok, _ in decoded_tokens[start:end])\n",
    "            if normalize(candidate) == target_string:\n",
    "                return [idx for _, idx in decoded_tokens[start:end]]\n",
    "    \n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8f61566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_span_vector(\n",
    "    lang,\n",
    "    tokenizer,\n",
    "    row_idx,\n",
    "    sentence,\n",
    "    tokens,\n",
    "    offsets,\n",
    "    hidden_states,\n",
    "    span_no_gap_words,\n",
    "    span_words,\n",
    "    use_all_layers=False,\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    # Fallback: span is missing\n",
    "    if not span_no_gap_words:\n",
    "        print(\"No span words provided.\")\n",
    "        dim = hidden_states[0].shape[-1]\n",
    "        return torch.zeros((len(hidden_states), dim)) if use_all_layers else torch.zeros((1, dim))\n",
    "    \n",
    "    token_list = tokenizer.convert_ids_to_tokens(tokens[row_idx])\n",
    "    offset_list = offsets[row_idx]\n",
    "\n",
    "    corrected_offsets = []\n",
    "    for token, (start, end) in zip(token_list, offset_list):\n",
    "        if token is None:\n",
    "            corrected_offsets.append((0, 0))\n",
    "            continue\n",
    "        if token.startswith(\"Ġ\") or token.startswith(\"▁\"):\n",
    "            corrected_offsets.append((start + 1, end))\n",
    "        else:\n",
    "            corrected_offsets.append((start, end))\n",
    "\n",
    "    # Match span\n",
    "    match = re.search(re.escape(span_words), sentence)\n",
    "    if not match:\n",
    "        dim = hidden_states[0].shape[-1]\n",
    "        return torch.zeros((len(hidden_states), dim)) if use_all_layers else torch.zeros((1, dim))\n",
    "\n",
    "    char_span = (match.start(), match.end())\n",
    "\n",
    "    span_token_indices = [\n",
    "        i for i, (start, end) in enumerate(corrected_offsets)\n",
    "        if start >= char_span[0] and end <= char_span[1]\n",
    "    ]\n",
    "\n",
    "    if not span_token_indices:\n",
    "        dim = hidden_states[0].shape[-1]\n",
    "        return torch.zeros((len(hidden_states), dim)) if use_all_layers else torch.zeros((1, dim))\n",
    "\n",
    "    target_tokens = [normalize(t) for t in span_no_gap_words]\n",
    "\n",
    "\n",
    "    target_tokens = [tokenizer.decode(tokenizer.encode(t, add_special_tokens=False)).strip().lower() for t in target_tokens]\n",
    "    \n",
    "    decoded_tokens = [\n",
    "        (normalize(tokenizer.convert_tokens_to_string([token_list[i]])), i)\n",
    "        for i in span_token_indices\n",
    "    ]\n",
    "\n",
    "\n",
    "    # If Japanese\n",
    "    if lang == \"japanese\":\n",
    "        selected_indices = match_japanese_span_tokens(decoded_tokens, span_no_gap_words)\n",
    "        chosen_tokens = [tok for tok, idx in decoded_tokens if idx in selected_indices]\n",
    "    else:\n",
    "        # Match subword groups\n",
    "        selected_indices = group_subword_matches(decoded_tokens, target_tokens.copy())\n",
    "\n",
    "    chosen_tokens = [tokenizer.convert_tokens_to_string([token_list[i]]).strip().lower() for i in selected_indices]\n",
    "\n",
    "\n",
    "    if not selected_indices:\n",
    "        dim = hidden_states[0].shape[-1]\n",
    "        return torch.zeros((len(hidden_states), dim)) if use_all_layers else torch.zeros((1, dim))\n",
    "\n",
    "    # Remove punctuation-only tokens from end\n",
    "    while selected_indices:\n",
    "        idx = selected_indices[-1]\n",
    "        tok = tokenizer.convert_tokens_to_string([token_list[idx]]).strip()\n",
    "        if re.fullmatch(r\"\\W+\", tok):  # only non-word characters\n",
    "            selected_indices.pop()\n",
    "            chosen_tokens.pop()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # --- Debugging ---\n",
    "    # Normalize and compare the matched tokens against the expected ones\n",
    "    matched_text = ''.join(chosen_tokens).replace(\" \", \"\")\n",
    "    expected_text = ''.join(target_tokens).replace(\" \", \"\")\n",
    "    if matched_text != expected_text:\n",
    "        print(f\"⚠️ Mismatch in span words for row {row_idx}:\")\n",
    "\n",
    "        print(f\"sentence: {sentence}\")\n",
    "        print(f\"span_no_gap_words: {span_no_gap_words}\")\n",
    "        print(f\"span_token_indices: {span_token_indices}\")\n",
    "        print(f\"Target tokens: {target_tokens}\")\n",
    "        print(f\"Decoded tokens: {decoded_tokens}\")\n",
    "        print(f\"chosen_tokens: {chosen_tokens}\")\n",
    "        print(f\"selected indices: {selected_indices}\")\n",
    "        print()\n",
    "\n",
    "    # --- Extract vector(s) ---\n",
    "    if use_all_layers:\n",
    "        layer_means = []\n",
    "        for layer in hidden_states:\n",
    "            span_vecs = layer[row_idx, selected_indices, :]\n",
    "            mean_vec = span_vecs.mean(dim=0)\n",
    "            layer_means.append(mean_vec.to(dtype=torch.float32).cpu())\n",
    "        return torch.stack(layer_means, dim=0)\n",
    "    else:\n",
    "        span_vecs = hidden_states[0][row_idx, selected_indices, :]\n",
    "        mean_vec = span_vecs.mean(dim=0)\n",
    "        return mean_vec.unsqueeze(0).to(dtype=torch.float32).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "89e37f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 7/7 [00:02<00:00,  3.43it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for i, batch in enumerate(tqdm(dataloader, desc=\"Processing batches\")):\n",
    "        batch_dict = {k: v.to(model.device) for k, v in zip(keys, batch)}\n",
    "        outputs = model(\n",
    "            **batch_dict,\n",
    "            output_hidden_states=True,\n",
    "            output_attentions=False,\n",
    "            use_cache=False  # disables `past_key_values`\n",
    "        )\n",
    "        del batch_dict\n",
    "\n",
    "        if LAYER_WISE_PROBING:\n",
    "            hidden_states = outputs.hidden_states  # tuple/list of tensors\n",
    "        else:\n",
    "            # Wrap the last hidden state in a list to maintain consistent structure\n",
    "            hidden_states = [outputs.hidden_states[-1]]\n",
    "\n",
    "        del outputs\n",
    "\n",
    "        # Determine the actual batch slice in the dataset\n",
    "        start_idx = i * BSZ\n",
    "        # FIX: Access the actual tensor shape, not the list/tuple shape\n",
    "        batch_size = hidden_states[0].shape[0]  # Get batch size from the first layer's tensor\n",
    "        end_idx = start_idx + batch_size\n",
    "        offset_batch = offset_mapping[start_idx:end_idx]\n",
    "        token_batch = tokenized[\"input_ids\"][start_idx:end_idx]\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            row = data.iloc[start_idx + j]\n",
    "\n",
    "            mean_vec = extract_span_vector(\n",
    "                lang=LANG,\n",
    "                tokenizer=tokenizer,\n",
    "                row_idx=j,\n",
    "                span_words=row[\"pie\"],\n",
    "                sentence=row[\"sentence\"],\n",
    "                tokens=token_batch,\n",
    "                offsets=offset_batch,\n",
    "                hidden_states=hidden_states,\n",
    "                span_no_gap_words=row[\"idiom_tokens\"],\n",
    "                # no_gap_words=NO_GAP_WORDS,\n",
    "                use_all_layers=LAYER_WISE_PROBING,\n",
    "            )\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c91d38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58488557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf296e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
